\documentclass[12pt]{amsproc}

\usepackage{amsmath, amsthm, ulem, graphicx, marvosym, fancyhdr, amscd, amssymb, enumitem, mathrsfs, multicol, setspace}
\usepackage{enumitem}
\usepackage{url}


\usepackage{listings, color}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}

\lstset{style=mystyle}


\newcommand\Z{{\mathbb Z}}
\newcommand\F{{\mathbb F}}
\newcommand\N{{\mathbb N}}
\newcommand\A{{\mathbf A}}
\newcommand\p{{\mathscr P}}
\newcommand\R{{\mathbb R}}
\newcommand\Q{{\mathbb Q}}
\newcommand\C{{\mathbb C}}
\newcommand\bfx{{\mathbf x}}
\newcommand\bfv{{\mathbf v}}
\newcommand\bfy{{\mathbf y}}
\newcommand\bfw{{\mathbf w}}

%\usepackage{graphics}

\topmargin -.7in
\evensidemargin-0in
\oddsidemargin0in
\textheight 9.5in
\textwidth 6.5in

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{cor}{Corollary}
\theoremstyle{definition}
\newtheorem{defi}{Definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{rmk}{Remark}
\newtheorem{interpret}{Interpretation}
%\theoremstyle{test}
\newtheorem{test}{Test}

\everymath={\displaystyle}

\pagestyle{empty}
\onehalfspacing

\title{Proposal: Titanic Survivability}

\author{
	Pacific Lutheran University  \\
	Miguel Amezola,
	Nicholas Glover,
	\and Quinton Teas
}
%	Pacific Lutheran University  \\
%	%	\and
%	%	Your friend who worked with you \\
%	%	His/her Major / University \\
%}

\date{\today}

\begin{document}
	
	\maketitle
		
	\begin{abstract}
	We will use passenger data from the sinking of the RMS Titanic to predict survival. Creating a feature space we give us the opportunity to experiment with feature generation. Implementing C4.5, a decision tree algorithm for continuous features and binary classification, will deepen our understanding of machine learning models.
	\end{abstract}
	
	\section{Introduction}\label{intro}
	The RMS Titanic sank on 15 April 1912, after colliding with an iceberg during its maiden voyage from Southampton to New York. Out of 2224 passengers and crew, 1502 lost their lives. Interestingly, some groups of people were more likely to survive, such as women, children, and the upper class. We would like to identify more factors that improved the likelihood of survival. Furthermore, we will map these factors to an appropriate feature space. And finally, we will implement and train a Decision Tree to analyze what sorts of people were likely to survive.
	
	\section{Implementation}\label{implementation}
	
	For the passengers on the Titanic, there were only two outcomes, namely, survive and not survive. Thus, they can be grouped into two disjoint sets or classes. We will use a decision tree to predict the class two which each passenger belongs.

	\subsection{Learning Model}\label{implementation:learning model}
	
	There are many decision-tree algorithms. Notable ones include ID3 (Iterative Dichotomiser 3) and C4.5 (successor to ID3) \cite{wikipedia:decision_tree}. 
	
	Since C4.5 made a number of important improvements to ID3, like the ability to handle both discreet and continuous attributes and allowing attributes to be marked ? for missing, we use this algorithm.
	
	This will require several subtasks, namely,
	
	\begin{enumerate}
		\item computing entropy gain,
		\item creating a decision node, and
		\item recursively splitting (binary) the feature space. 
	\end{enumerate}
	
	Each task is assigned to a different group member: (1) Miguel, (2) Nicholas, and (3) Quinton. 


	\begin{table}[h!]
		\centering
		\caption{Tentative Schedule}
		\label{Tentative Schedule}
		\begin{tabular}{lll}
			\textbf{Start Date} & \textbf{Duration} & \textbf{Task} \\
			April 10 or April 12 & N/A & Project Proposal Presentation \\
			April 3 & 1 week & Generate features. \\
			April 10 & 2 week & Implement Decision Tree \\
			April 24 & 1 week & Train/Test \\
			May 1 & 1 week & Predict \\
			May 8 & 1 week & Prepare final presentation \\
			May 8 & 2 weeks & Prepare report \\
			May 15, 17, or 19 & N/A & Final project presentation \\
			May 24 & N/A & Report due
		\end{tabular}
	\end{table}
	
	
	\section{Method}\label{method}
	
	\subsection{Data}\label{method:data}
	The data have been partitioned into two disjoint subsets, $E$ and $F$, such that the cardinality of $F$ is about one half the cardinality of $E$; that is, $|F| \approx \frac{1}{2} |E|$. As demonstrated in Table \ref{table:data dictionary}, the data have ten variables for each passenger. Note that \textbf{pclass} is a proxy for the socio-economic status of the passenger. Also, \textbf{age} is fractional if less than one, or in the form of $xx.5$ if the age was estimated. For \textbf{sibsp}, a sibling is defined as a brother, sister, stepbrother, stepsister, and a spouse is defined as a husband, wife (mistresses and fianc\'{e}s were ignored). Similarly for \textbf{parch}, a parent is a mother or father and a child is a daughter, son, stepdaughter, or stepson. If children traveled with a nanny, then parch = 0 for them \cite{kaggle}.
	 

	\begin{table}[h]
		\centering
		\caption{Data Dictionary}
		\label{table:data dictionary}
		\begin{tabular}{ll}
			\textbf{Variable} & \textbf{Definition} \\
			survival & Survival \\
			pclass & Ticket class \\
			sex & Sex \\
			age & Age in years \\
			sibsp & number of siblings or spouses aboard the Titanic \\
			parch & number of parents or children aboard the Titanic \\
			ticket & Ticket number \\
			fare & Passenger fare \\
			cabin & Cabin number \\
			embarked & Port of Embarkation
		\end{tabular}
	\end{table}
	
	
	\subsection{Train}\label{method:train}
	The larger subset of data, $E$, will be used to build the machine learning model.
	
	\subsection{Test}\label{method:test}
	We will use dataset $F$ to see how well the model performs on unseen data. 
	
	\subsection{Predict}\label{method:predict}
	
%	\subsubsection{Goal}\label{method:predict:goal}
%	We will predict whether or not a passenger survived the sinking of the Titanic. 

	Predictions will be recorded in a csv file with exactly 418 entries and a header row, as in Listing 1. The file will have exactly 2 columns:
	\begin{itemize}
		\item PassengerId (sorted in any order), and
		\item Survived (contains binary predictions: 1 for survived, 0 for deceased).
	\end{itemize}
	
	\begin{lstlisting}[language=Python, caption=Prediction example]
		PassengerId,Survived
		892,0
		893,1
		894,0
		Etc.\end{lstlisting}
		
	We will use the following metric to see how well the model performed.

	\begin{definition}[Accuracy]\label{definition:accuracy}
		Let $c,i \in \N$ such that $c + i \neq 0$ with $c$ equal to the number of correct predictions and $i$ equal to the number of incorrect predictions, and let $\mathscr{A}: \N \to [0,1] \cap \R$ be the function defined by   
		$$\mathscr{A}(c,i) := \frac{c}{c + i}.$$
		This function is a statistical measure known as accuracy. 
	\end{definition}
	
	\section{Conclusion}\label{conclusions}
	Not only is the sinking of the RMS Titanic an interesting historical event, but an opportunity to design a machine learning model and test its performance. Since we are familiar with ID3, implementing its successor C4.5 will allow us to build on previous knowledge. The hands-on experience stemming from making our own implementation will deepen our understanding of machine learning models.

		
	\bibliographystyle{plain}
	\bibliography{refs.bib}
	
	

\end{document}